# Adam-Optimizer
This repository shows you the working of the adam optimizer.
Adam optimiser is a much more effecient algorithm than gradient descent.
Although most of the modern deep learning framworks would have adam optimiser as a in-built optmiser, it is good to build an intuition as to how it works.
This repository is from course 2 of Deep Learning speciliazation on Coursera.
